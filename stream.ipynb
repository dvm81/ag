{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¢ Entity Recognition Tool - Step by Step Tutorial\n",
    "\n",
    "This notebook demonstrates how to build a complete company entity recognition system from scratch, starting with basic text processing and progressively adding more advanced features.\n",
    "\n",
    "## üìã What We'll Build:\n",
    "\n",
    "1. **Step 1**: Basic entity extraction from text\n",
    "2. **Step 2**: Simple Streamlit app for text processing\n",
    "3. **Step 3**: Add PDF processing functionality\n",
    "4. **Step 4**: Add image OCR capabilities\n",
    "5. **Step 5**: Add AI vision analysis\n",
    "6. **Step 6**: Final comprehensive application\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Initial Setup\n",
    "\n",
    "First, let's import all the libraries we'll need and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API key - try multiple approaches\n",
    "API_KEY = None\n",
    "\n",
    "# Try to get from environment\n",
    "env_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if env_key and len(env_key) > 10 and env_key.startswith(\"sk-\"):\n",
    "    API_KEY = env_key\n",
    "    print(\"‚úÖ Using API key from environment variable\")\n",
    "else:\n",
    "    print(\"‚ùå No valid API key found in environment\")\n",
    "    print(\"üìù Please set OPENAI_API_KEY in your .env file or environment\")\n",
    "    print(\"üîó Get your API key from: https://platform.openai.com/api-keys\")\n",
    "\n",
    "if API_KEY:\n",
    "    # Verify the key format\n",
    "    if not API_KEY.startswith(\"sk-\"):\n",
    "        print(\"‚ö†Ô∏è Warning: API key should start with 'sk-'\")\n",
    "    else:\n",
    "        print(f\"‚úÖ API key format looks correct (starts with: {API_KEY[:10]}...)\")\n",
    "else:\n",
    "    print(\"‚ùå No API key available - please set it before proceeding\")\n",
    "    \n",
    "print(\"‚úÖ Initial setup complete!\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù Step 1: Entity Extraction with 3-Step Pipeline\n",
    "\n",
    "Let's start with building a complete entity recognition system using a multi-prompt pipeline:\n",
    "1. **Extract** - Find potential company mentions\n",
    "2. **Ground** - Match against company database  \n",
    "3. **Verify** - Use RAG-style verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set API key directly here for testing (uncomment and add your key)\n",
    "# API_KEY = \"sk-your-actual-api-key-here\"\n",
    "\n",
    "# Or check what's currently set\n",
    "print(\"Current API key status:\")\n",
    "if API_KEY:\n",
    "    print(f\"‚úÖ API key available: {API_KEY[:20]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No API key set\")\n",
    "    print(\"üí° You can:\")\n",
    "    print(\"   1. Create a .env file with OPENAI_API_KEY=your_key\")\n",
    "    print(\"   2. Set it directly in the cell above\")\n",
    "    print(\"   3. Export it in your terminal: export OPENAI_API_KEY=your_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API key and credits\n",
    "print(\"üß™ Testing API Key and Credits...\")\n",
    "\n",
    "if API_KEY:\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=API_KEY)\n",
    "        \n",
    "        print(f\"üîë Testing key: {API_KEY[:20]}...\")\n",
    "        \n",
    "        # Make a minimal API call to test\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'API test successful'\"}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ API Test Successful!\")\n",
    "        print(f\"üìù Response: {response.choices[0].message.content}\")\n",
    "        print(\"üí≥ Credits available - ready to proceed!\")\n",
    "        \n",
    "        # Test with the model we'll actually use\n",
    "        print(\"\\nüî¨ Testing gpt-4o-mini model...\")\n",
    "        response2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n",
    "            max_tokens=5\n",
    "        )\n",
    "        print(\"‚úÖ gpt-4o-mini model working!\")\n",
    "        print(\"üöÄ Ready for entity extraction pipeline!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API Error: {e}\")\n",
    "        error_str = str(e).lower()\n",
    "        \n",
    "        if \"401\" in error_str or \"unauthorized\" in error_str:\n",
    "            print(\"üîë Issue: Invalid API key\")\n",
    "            print(\"üí° Solution: Check your API key at https://platform.openai.com/api-keys\")\n",
    "        elif \"insufficient_quota\" in error_str or \"quota\" in error_str:\n",
    "            print(\"üí≥ Issue: No credits remaining\")\n",
    "            print(\"üí° Solution: Add billing at https://platform.openai.com/account/billing\")\n",
    "        elif \"rate_limit\" in error_str:\n",
    "            print(\"‚è∞ Issue: Rate limit exceeded\")\n",
    "            print(\"üí° Solution: Wait a moment and try again\")\n",
    "        else:\n",
    "            print(\"üîß Unknown API issue\")\n",
    "            print(\"üí° Solution: Check https://status.openai.com/ for service status\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå No API key found\")\n",
    "    print(\"üí° Please set your API key in the previous cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# First, let's create a simple single-prompt extractor for comparison\n",
    "class SimpleEntityExtractor:\n",
    "    \"\"\"Simple entity extractor using OpenAI API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key is required\")\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    def extract_companies(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract company mentions from text\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Extract all company names and stock tickers from the following text.\n",
    "        Return the result as a JSON object with a 'companies' array.\n",
    "        \n",
    "        Text: {text}\n",
    "        \n",
    "        Format: {{\"companies\": [{{\"name\": \"Company Name\", \"ticker\": \"TICK\", \"original_text\": \"as found in text\"}}]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a financial entity extraction expert. Always respond with valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result.get('companies', [])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting entities: {e}\")\n",
    "            return []\n",
    "\n",
    "# Create simple extractor instance if API key is available\n",
    "if API_KEY:\n",
    "    try:\n",
    "        simple_extractor = SimpleEntityExtractor(API_KEY)\n",
    "        print(\"‚úÖ Simple entity extractor created!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating simple extractor: {e}\")\n",
    "        simple_extractor = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping simple extractor creation - no API key\")\n",
    "    simple_extractor = None\n",
    "\n",
    "# Now let's create the advanced 3-step pipeline\n",
    "class CompanyDatabase:\n",
    "    \"\"\"Simple company database for entity grounding\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Hardcoded database of known companies\n",
    "        self.companies = [\n",
    "            {\"name\": \"Apple Inc.\", \"ticker\": \"AAPL\", \"aliases\": [\"Apple\", \"AAPL\"], \"sector\": \"Technology\"},\n",
    "            {\"name\": \"Microsoft Corporation\", \"ticker\": \"MSFT\", \"aliases\": [\"Microsoft\", \"MSFT\", \"MS\"], \"sector\": \"Technology\"},\n",
    "            {\"name\": \"Tesla, Inc.\", \"ticker\": \"TSLA\", \"aliases\": [\"Tesla\", \"TSLA\"], \"sector\": \"Automotive\"},\n",
    "            {\"name\": \"Alphabet Inc.\", \"ticker\": \"GOOGL\", \"aliases\": [\"Google\", \"Alphabet\", \"GOOGL\", \"GOOG\"], \"sector\": \"Technology\"},\n",
    "            {\"name\": \"NVIDIA Corporation\", \"ticker\": \"NVDA\", \"aliases\": [\"NVIDIA\", \"Nvidia\", \"NVDA\"], \"sector\": \"Technology\"},\n",
    "            {\"name\": \"Meta Platforms, Inc.\", \"ticker\": \"META\", \"aliases\": [\"Meta\", \"Facebook\", \"META\", \"FB\"], \"sector\": \"Technology\"},\n",
    "            {\"name\": \"Amazon.com, Inc.\", \"ticker\": \"AMZN\", \"aliases\": [\"Amazon\", \"AMZN\"], \"sector\": \"E-commerce\"},\n",
    "            {\"name\": \"Netflix, Inc.\", \"ticker\": \"NFLX\", \"aliases\": [\"Netflix\", \"NFLX\"], \"sector\": \"Entertainment\"},\n",
    "            {\"name\": \"Adobe Inc.\", \"ticker\": \"ADBE\", \"aliases\": [\"Adobe\", \"ADBE\"], \"sector\": \"Software\"},\n",
    "            {\"name\": \"Intel Corporation\", \"ticker\": \"INTC\", \"aliases\": [\"Intel\", \"INTC\"], \"sector\": \"Technology\"}\n",
    "        ]\n",
    "    \n",
    "    def search_companies(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for companies matching query\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        matches = []\n",
    "        \n",
    "        for company in self.companies:\n",
    "            # Check if query matches name, ticker, or aliases\n",
    "            if (query_lower in company['name'].lower() or \n",
    "                query_lower == company['ticker'].lower() or\n",
    "                any(query_lower in alias.lower() for alias in company['aliases'])):\n",
    "                matches.append(company)\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def get_all_companies(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all companies in database\"\"\"\n",
    "        return self.companies\n",
    "\n",
    "class AdvancedEntityExtractor:\n",
    "    \"\"\"Advanced entity extractor with multi-prompt pipeline and deduplication\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key is required\")\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.company_db = CompanyDatabase()\n",
    "    \n",
    "    def _deduplicate_extracted_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Remove duplicate entities from step 1 extraction\"\"\"\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            # Create a key based on text (case insensitive)\n",
    "            key = entity['text'].lower().strip()\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                deduped.append(entity)\n",
    "        \n",
    "        return deduped\n",
    "    \n",
    "    def _deduplicate_grounded_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Remove duplicate entities from step 2 grounding based on ticker\"\"\"\n",
    "        seen_tickers = set()\n",
    "        deduped = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            ticker = entity['matched_company']['ticker']\n",
    "            if ticker not in seen_tickers:\n",
    "                seen_tickers.add(ticker)\n",
    "                deduped.append(entity)\n",
    "            else:\n",
    "                # If we see the same ticker again, keep the one with higher confidence\n",
    "                existing_idx = next(i for i, e in enumerate(deduped) \n",
    "                                  if e['matched_company']['ticker'] == ticker)\n",
    "                if entity['confidence'] > deduped[existing_idx]['confidence']:\n",
    "                    deduped[existing_idx] = entity\n",
    "        \n",
    "        return deduped\n",
    "    \n",
    "    def _deduplicate_verified_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Remove duplicate entities from step 3 verification based on ticker\"\"\"\n",
    "        seen_tickers = set()\n",
    "        deduped = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            ticker = entity['ticker']\n",
    "            if ticker not in seen_tickers:\n",
    "                seen_tickers.add(ticker)\n",
    "                deduped.append(entity)\n",
    "            else:\n",
    "                # If we see the same ticker again, keep the one with higher confidence\n",
    "                existing_idx = next(i for i, e in enumerate(deduped) \n",
    "                                  if e['ticker'] == ticker)\n",
    "                if entity['confidence'] > deduped[existing_idx]['confidence']:\n",
    "                    deduped[existing_idx] = entity\n",
    "        \n",
    "        return deduped\n",
    "    \n",
    "    def step1_extract_entities(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Step 1: Extract potential company mentions from text\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        STEP 1: ENTITY EXTRACTION\n",
    "        \n",
    "        Extract ALL potential company names, brand names, and stock tickers from the following text.\n",
    "        Be liberal in extraction - include anything that might be a company.\n",
    "        IMPORTANT: Do not include duplicates in your response.\n",
    "        \n",
    "        Text: {text}\n",
    "        \n",
    "        Return JSON format:\n",
    "        {{\n",
    "            \"entities\": [\n",
    "                {{\n",
    "                    \"text\": \"extracted text exactly as found\",\n",
    "                    \"type\": \"company_name|ticker|brand\",\n",
    "                    \"context\": \"surrounding context (10-15 words)\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert at extracting company mentions from text. Be thorough but avoid duplicates.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            entities = result.get('entities', [])\n",
    "            \n",
    "            # Apply deduplication\n",
    "            deduped_entities = self._deduplicate_extracted_entities(entities)\n",
    "            \n",
    "            return deduped_entities\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in step 1: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def step2_ground_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Step 2: Ground entities against company database\"\"\"\n",
    "        \n",
    "        grounded_entities = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            # Search in company database\n",
    "            matches = self.company_db.search_companies(entity['text'])\n",
    "            \n",
    "            if matches:\n",
    "                # Take the best match (first one for now)\n",
    "                best_match = matches[0]\n",
    "                grounded_entities.append({\n",
    "                    'original': entity['text'],\n",
    "                    'type': entity['type'],\n",
    "                    'context': entity.get('context', ''),\n",
    "                    'matched_company': best_match,\n",
    "                    'match_type': 'database_exact' if entity['text'].lower() == best_match['ticker'].lower() else 'database_fuzzy',\n",
    "                    'confidence': 95 if entity['text'].lower() == best_match['ticker'].lower() else 85\n",
    "                })\n",
    "        \n",
    "        # Apply deduplication based on ticker\n",
    "        deduped_entities = self._deduplicate_grounded_entities(grounded_entities)\n",
    "        \n",
    "        return deduped_entities\n",
    "    \n",
    "    def step3_verify_entities(self, grounded_entities: List[Dict[str, Any]], original_text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Step 3: Use RAG-style verification with company database context\"\"\"\n",
    "        \n",
    "        if not grounded_entities:\n",
    "            return []\n",
    "        \n",
    "        # Prepare company database context\n",
    "        db_context = \"Company Database Context:\\n\"\n",
    "        for company in self.company_db.get_all_companies():\n",
    "            db_context += f\"- {company['name']} ({company['ticker']}): {', '.join(company['aliases'][:3])}\\n\"\n",
    "        \n",
    "        # Prepare entities for verification\n",
    "        entities_text = \"\\n\".join([\n",
    "            f\"- '{e['original']}' ‚Üí {e['matched_company']['name']} ({e['matched_company']['ticker']})\"\n",
    "            for e in grounded_entities\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        STEP 3: ENTITY VERIFICATION (RAG-style)\n",
    "        \n",
    "        Given the company database context below, verify and refine the extracted entities.\n",
    "        IMPORTANT: Return only unique companies (no duplicates by ticker).\n",
    "        \n",
    "        {db_context}\n",
    "        \n",
    "        Original Text: {original_text}\n",
    "        \n",
    "        Extracted and Matched Entities:\n",
    "        {entities_text}\n",
    "        \n",
    "        For each entity, verify it's correctly matched and actually mentioned in the original text.\n",
    "        Rate confidence 1-100 based on:\n",
    "        - Exact ticker match: 95-100\n",
    "        - Company name match: 85-95  \n",
    "        - Alias/brand match: 75-85\n",
    "        - Contextual relevance: +/- 10\n",
    "        \n",
    "        Return JSON format (NO DUPLICATES):\n",
    "        {{\n",
    "            \"verified_entities\": [\n",
    "                {{\n",
    "                    \"original\": \"text as found\",\n",
    "                    \"company_name\": \"official company name\",\n",
    "                    \"ticker\": \"stock ticker\",\n",
    "                    \"confidence\": 85,\n",
    "                    \"reasoning\": \"why this match is correct\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a financial entity verification expert. Use the company database to verify entities and assign accurate confidence scores. Return unique entities only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            entities = result.get('verified_entities', [])\n",
    "            \n",
    "            # Apply final deduplication\n",
    "            deduped_entities = self._deduplicate_verified_entities(entities)\n",
    "            \n",
    "            return deduped_entities\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in step 3: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_text_pipeline(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Complete 3-step pipeline: Extract ‚Üí Ground ‚Üí Verify with deduplication\"\"\"\n",
    "        \n",
    "        print(\"üîç Step 1: Extracting potential entities...\")\n",
    "        extracted = self.step1_extract_entities(text)\n",
    "        print(f\"   Found {len(extracted)} unique potential entities\")\n",
    "        \n",
    "        print(\"üóÉÔ∏è Step 2: Grounding against company database...\")\n",
    "        grounded = self.step2_ground_entities(extracted)\n",
    "        print(f\"   Grounded {len(grounded)} unique entities\")\n",
    "        \n",
    "        print(\"‚úÖ Step 3: Verifying with RAG-style analysis...\")\n",
    "        verified = self.step3_verify_entities(grounded, text)\n",
    "        print(f\"   Verified {len(verified)} unique final entities\")\n",
    "        \n",
    "        return {\n",
    "            'step1_extracted': extracted,\n",
    "            'step2_grounded': grounded,\n",
    "            'step3_verified': verified,\n",
    "            'summary': f\"Pipeline processed {len(extracted)} ‚Üí {len(grounded)} ‚Üí {len(verified)} unique entities\"\n",
    "        }\n",
    "\n",
    "# Create advanced extractor instance if API key is available\n",
    "if API_KEY:\n",
    "    try:\n",
    "        advanced_extractor = AdvancedEntityExtractor(API_KEY)\n",
    "        print(\"‚úÖ Advanced entity extractor with deduplication created!\")\n",
    "        print(\"üìä Pipeline: Extract ‚Üí Database Lookup ‚Üí RAG Verification ‚Üí Deduplicate\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating advanced extractor: {e}\")\n",
    "        advanced_extractor = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping advanced extractor creation - no API key\")\n",
    "    advanced_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the advanced 3-step pipeline with detailed output\n",
    "sample_text = \"\"\"\n",
    "Apple (AAPL) reported strong Q4 earnings, with revenue reaching $89.5 billion. \n",
    "Microsoft (MSFT) also showed impressive results with Azure growing 35% year-over-year.\n",
    "Meanwhile, Tesla stock (TSLA) surged after announcing new production milestones.\n",
    "Google parent company Alphabet (GOOGL) saw advertising revenue rebound.\n",
    "Facebook's parent Meta announced new VR initiatives.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üöÄ Testing Advanced 3-Step Entity Recognition Pipeline with Deduplication\")\n",
    "print(f\"üìÑ Text: {sample_text.strip()}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Check if extractor is available\n",
    "if advanced_extractor is None:\n",
    "    print(\"‚ùå Advanced extractor not available. Please run the previous cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Run the complete pipeline\n",
    "        pipeline_result = advanced_extractor.process_text_pipeline(sample_text)\n",
    "\n",
    "        print(f\"\\nüìä {pipeline_result['summary']}\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "        # Show detailed results for each step\n",
    "        print(\"\\nüîç STEP 1 - Raw Extraction:\")\n",
    "        for i, entity in enumerate(pipeline_result['step1_extracted'], 1):\n",
    "            print(f\"  {i}. '{entity['text']}' ({entity['type']})\")\n",
    "\n",
    "        print(f\"\\nüóÉÔ∏è STEP 2 - Database Grounding:\")\n",
    "        for i, entity in enumerate(pipeline_result['step2_grounded'], 1):\n",
    "            print(f\"  {i}. '{entity['original']}' ‚Üí {entity['matched_company']['ticker']} ({entity['confidence']}%)\")\n",
    "\n",
    "        print(f\"\\n‚úÖ STEP 3 - Final Verification:\")\n",
    "        if len(pipeline_result['step3_verified']) > 5:\n",
    "            print(f\"‚ö†Ô∏è WARNING: Found {len(pipeline_result['step3_verified'])} entities - checking for duplicates...\")\n",
    "            \n",
    "            # Check for duplicates by ticker\n",
    "            tickers_seen = {}\n",
    "            for i, entity in enumerate(pipeline_result['step3_verified'], 1):\n",
    "                ticker = entity['ticker']\n",
    "                if ticker in tickers_seen:\n",
    "                    print(f\"  üî¥ DUPLICATE: {ticker} found at positions {tickers_seen[ticker]} and {i}\")\n",
    "                else:\n",
    "                    tickers_seen[ticker] = i\n",
    "                print(f\"  {i}. {entity['company_name']} ({entity['ticker']}) - '{entity['original']}' ({entity['confidence']}%)\")\n",
    "        else:\n",
    "            for i, entity in enumerate(pipeline_result['step3_verified'], 1):\n",
    "                print(f\"  {i}. {entity['company_name']} ({entity['ticker']}) - '{entity['original']}' ({entity['confidence']}%)\")\n",
    "\n",
    "        print(f\"\\nüéØ Final Result: {len(pipeline_result['step3_verified'])} entities\")\n",
    "        \n",
    "        # Additional analysis\n",
    "        if len(pipeline_result['step3_verified']) > 5:\n",
    "            print(\"\\nüîß DEBUGGING INFO:\")\n",
    "            unique_tickers = set(e['ticker'] for e in pipeline_result['step3_verified'])\n",
    "            print(f\"   Unique tickers found: {len(unique_tickers)}\")\n",
    "            print(f\"   All tickers: {list(unique_tickers)}\")\n",
    "            \n",
    "            if len(unique_tickers) < len(pipeline_result['step3_verified']):\n",
    "                print(\"   ‚ùå Deduplication failed - multiple entries per ticker\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ No ticker duplicates, but more entities than expected\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running pipeline: {e}\")\n",
    "        print(\"This may be due to API issues or missing dependencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the advanced 3-step pipeline with AGGRESSIVE deduplication\n",
    "sample_text = \"\"\"\n",
    "Apple (AAPL) reported strong Q4 earnings, with revenue reaching $89.5 billion. \n",
    "Microsoft (MSFT) also showed impressive results with Azure growing 35% year-over-year.\n",
    "Meanwhile, Tesla stock (TSLA) surged after announcing new production milestones.\n",
    "Google parent company Alphabet (GOOGL) saw advertising revenue rebound.\n",
    "Facebook's parent Meta announced new VR initiatives.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üöÄ Testing Advanced 3-Step Entity Recognition Pipeline with AGGRESSIVE Deduplication\")\n",
    "print(f\"üìÑ Text: {sample_text.strip()}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Check if extractor is available\n",
    "if advanced_extractor is None:\n",
    "    print(\"‚ùå Advanced extractor not available. Please run the previous cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Run the complete pipeline\n",
    "        pipeline_result = advanced_extractor.process_text_pipeline(sample_text)\n",
    "\n",
    "        print(f\"\\nüìä {pipeline_result['summary']}\")\n",
    "        \n",
    "        # Apply additional aggressive deduplication\n",
    "        original_count = len(pipeline_result['step3_verified'])\n",
    "        pipeline_result['step3_verified'] = force_deduplicate_final_results(pipeline_result['step3_verified'])\n",
    "        final_count = len(pipeline_result['step3_verified'])\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "        # Show detailed results for each step\n",
    "        print(\"\\nüîç STEP 1 - Raw Extraction:\")\n",
    "        for i, entity in enumerate(pipeline_result['step1_extracted'], 1):\n",
    "            print(f\"  {i}. '{entity['text']}' ({entity['type']})\")\n",
    "\n",
    "        print(f\"\\nüóÉÔ∏è STEP 2 - Database Grounding:\")\n",
    "        for i, entity in enumerate(pipeline_result['step2_grounded'], 1):\n",
    "            print(f\"  {i}. '{entity['original']}' ‚Üí {entity['matched_company']['ticker']} ({entity['confidence']}%)\")\n",
    "\n",
    "        print(f\"\\n‚úÖ STEP 3 - Final Verification (after aggressive deduplication):\")\n",
    "        for i, entity in enumerate(pipeline_result['step3_verified'], 1):\n",
    "            print(f\"  {i}. {entity['company_name']} ({entity['ticker']}) - '{entity['original']}' ({entity['confidence']}%)\")\n",
    "            print(f\"     Reasoning: {entity.get('reasoning', 'N/A')}\")\n",
    "\n",
    "        print(f\"\\nüéØ Final Result: {final_count} unique entities\")\n",
    "        \n",
    "        if original_count != final_count:\n",
    "            print(f\"üîß Deduplication removed {original_count - final_count} duplicates\")\n",
    "        \n",
    "        # Verify uniqueness\n",
    "        tickers = [e['ticker'] for e in pipeline_result['step3_verified']]\n",
    "        if len(tickers) == len(set(tickers)):\n",
    "            print(\"‚úÖ All entities are unique by ticker!\")\n",
    "        else:\n",
    "            print(\"‚ùå Still found duplicates - manual inspection needed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running pipeline: {e}\")\n",
    "        print(\"This may be due to API issues or missing dependencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the advanced 3-step pipeline\n",
    "sample_text = \"\"\"\n",
    "Apple (AAPL) reported strong Q4 earnings, with revenue reaching $89.5 billion. \n",
    "Microsoft (MSFT) also showed impressive results with Azure growing 35% year-over-year.\n",
    "Meanwhile, Tesla stock (TSLA) surged after announcing new production milestones.\n",
    "Google parent company Alphabet (GOOGL) saw advertising revenue rebound.\n",
    "Facebook's parent Meta announced new VR initiatives.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üöÄ Testing Advanced 3-Step Entity Recognition Pipeline\")\n",
    "print(f\"üìÑ Text: {sample_text.strip()}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Check if extractor is available\n",
    "if advanced_extractor is None:\n",
    "    print(\"‚ùå Advanced extractor not available. Please run the previous cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Run the complete pipeline\n",
    "        pipeline_result = advanced_extractor.process_text_pipeline(sample_text)\n",
    "\n",
    "        print(f\"\\nüìä {pipeline_result['summary']}\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "        # Show detailed results for each step\n",
    "        print(\"\\nüîç STEP 1 - Raw Extraction:\")\n",
    "        for i, entity in enumerate(pipeline_result['step1_extracted'], 1):\n",
    "            print(f\"  {i}. '{entity['text']}' ({entity['type']}) - Context: {entity['context'][:50]}...\")\n",
    "\n",
    "        print(f\"\\nüóÉÔ∏è STEP 2 - Database Grounding:\")\n",
    "        for i, entity in enumerate(pipeline_result['step2_grounded'], 1):\n",
    "            print(f\"  {i}. '{entity['original']}' ‚Üí {entity['matched_company']['name']} ({entity['matched_company']['ticker']}) - {entity['match_type']} ({entity['confidence']}%)\")\n",
    "\n",
    "        print(f\"\\n‚úÖ STEP 3 - Final Verification:\")\n",
    "        for i, entity in enumerate(pipeline_result['step3_verified'], 1):\n",
    "            print(f\"  {i}. {entity['company_name']} ({entity['ticker']}) - Found as: '{entity['original']}' (Confidence: {entity['confidence']}%)\")\n",
    "            print(f\"     Reasoning: {entity['reasoning']}\")\n",
    "\n",
    "        print(f\"\\nüéØ Final Result: {len(pipeline_result['step3_verified'])} verified companies\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running pipeline: {e}\")\n",
    "        print(\"This may be due to API issues or missing dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üñ•Ô∏è Step 2: Simple Streamlit App for Text Processing\n",
    "\n",
    "Now let's create a minimal Streamlit app that uses our entity extractor. We'll save this to a file and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced Streamlit app with 3-step pipeline\n",
    "streamlit_app_step2_enhanced = '''\n",
    "import streamlit as st\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# App configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Entity Extractor - Step 2 Enhanced\",\n",
    "    page_icon=\"üè¢\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "class CompanyDatabase:\n",
    "    \"\"\"Simple company database for entity grounding\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.companies = [\n",
    "            {\"name\": \"Apple Inc.\", \"ticker\": \"AAPL\", \"aliases\": [\"Apple\", \"AAPL\"]},\n",
    "            {\"name\": \"Microsoft Corporation\", \"ticker\": \"MSFT\", \"aliases\": [\"Microsoft\", \"MSFT\", \"MS\"]},\n",
    "            {\"name\": \"Tesla, Inc.\", \"ticker\": \"TSLA\", \"aliases\": [\"Tesla\", \"TSLA\"]},\n",
    "            {\"name\": \"Alphabet Inc.\", \"ticker\": \"GOOGL\", \"aliases\": [\"Google\", \"Alphabet\", \"GOOGL\", \"GOOG\"]},\n",
    "            {\"name\": \"NVIDIA Corporation\", \"ticker\": \"NVDA\", \"aliases\": [\"NVIDIA\", \"Nvidia\", \"NVDA\"]},\n",
    "            {\"name\": \"Meta Platforms, Inc.\", \"ticker\": \"META\", \"aliases\": [\"Meta\", \"Facebook\", \"META\", \"FB\"]},\n",
    "            {\"name\": \"Amazon.com, Inc.\", \"ticker\": \"AMZN\", \"aliases\": [\"Amazon\", \"AMZN\"]},\n",
    "            {\"name\": \"Netflix, Inc.\", \"ticker\": \"NFLX\", \"aliases\": [\"Netflix\", \"NFLX\"]},\n",
    "            {\"name\": \"Adobe Inc.\", \"ticker\": \"ADBE\", \"aliases\": [\"Adobe\", \"ADBE\"]},\n",
    "            {\"name\": \"Intel Corporation\", \"ticker\": \"INTC\", \"aliases\": [\"Intel\", \"INTC\"]}\n",
    "        ]\n",
    "    \n",
    "    def search_companies(self, query: str) -> List[Dict[str, Any]]:\n",
    "        query_lower = query.lower()\n",
    "        matches = []\n",
    "        for company in self.companies:\n",
    "            if (query_lower in company['name'].lower() or \n",
    "                query_lower == company['ticker'].lower() or\n",
    "                any(query_lower in alias.lower() for alias in company['aliases'])):\n",
    "                matches.append(company)\n",
    "        return matches\n",
    "    \n",
    "    def get_all_companies(self) -> List[Dict[str, Any]]:\n",
    "        return self.companies\n",
    "\n",
    "class AdvancedEntityExtractor:\n",
    "    \"\"\"Advanced entity extractor with multi-prompt pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.company_db = CompanyDatabase()\n",
    "    \n",
    "    def step1_extract_entities(self, text: str) -> List[Dict[str, Any]]:\n",
    "        prompt = f\"\"\"\n",
    "        STEP 1: ENTITY EXTRACTION\n",
    "        Extract ALL potential company names, brand names, and stock tickers from the text.\n",
    "        Text: {text}\n",
    "        Return JSON: {{\"entities\": [{{\"text\": \"name\", \"type\": \"company_name|ticker|brand\", \"context\": \"context\"}}]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Extract all potential company mentions thoroughly.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result.get('entities', [])\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error in step 1: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def step2_ground_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        grounded_entities = []\n",
    "        for entity in entities:\n",
    "            matches = self.company_db.search_companies(entity['text'])\n",
    "            if matches:\n",
    "                best_match = matches[0]\n",
    "                grounded_entities.append({\n",
    "                    'original': entity['text'],\n",
    "                    'type': entity['type'],\n",
    "                    'context': entity['context'],\n",
    "                    'matched_company': best_match,\n",
    "                    'match_type': 'exact' if entity['text'].lower() == best_match['ticker'].lower() else 'fuzzy',\n",
    "                    'confidence': 95 if entity['text'].lower() == best_match['ticker'].lower() else 85\n",
    "                })\n",
    "        return grounded_entities\n",
    "    \n",
    "    def step3_verify_entities(self, grounded_entities: List[Dict[str, Any]], original_text: str) -> List[Dict[str, Any]]:\n",
    "        if not grounded_entities:\n",
    "            return []\n",
    "        \n",
    "        db_context = \"Company Database:\\\\n\"\n",
    "        for company in self.company_db.get_all_companies()[:10]:\n",
    "            db_context += f\"- {company['name']} ({company['ticker']})\\\\n\"\n",
    "        \n",
    "        entities_text = \"\\\\n\".join([\n",
    "            f\"- {e['original']} ‚Üí {e['matched_company']['name']} ({e['matched_company']['ticker']})\"\n",
    "            for e in grounded_entities\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        STEP 3: ENTITY VERIFICATION (RAG-style)\n",
    "        \n",
    "        {db_context}\n",
    "        \n",
    "        Original Text: {original_text}\n",
    "        Extracted Entities: {entities_text}\n",
    "        \n",
    "        Verify entities and return JSON:\n",
    "        {{\"verified_entities\": [{{\"original\": \"text\", \"company_name\": \"name\", \"ticker\": \"ticker\", \"confidence\": 85, \"reasoning\": \"why correct\"}}]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Verify entities using the company database context.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result.get('verified_entities', [])\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error in step 3: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_text_pipeline(self, text: str) -> Dict[str, Any]:\n",
    "        with st.status(\"Running 3-step pipeline...\", expanded=True) as status:\n",
    "            st.write(\"üîç Step 1: Extracting potential entities...\")\n",
    "            extracted = self.step1_extract_entities(text)\n",
    "            st.write(f\"   Found {len(extracted)} potential entities\")\n",
    "            \n",
    "            st.write(\"üóÉÔ∏è Step 2: Grounding against company database...\")\n",
    "            grounded = self.step2_ground_entities(extracted)\n",
    "            st.write(f\"   Grounded {len(grounded)} entities\")\n",
    "            \n",
    "            st.write(\"‚úÖ Step 3: Verifying with RAG-style analysis...\")\n",
    "            verified = self.step3_verify_entities(grounded, text)\n",
    "            st.write(f\"   Verified {len(verified)} final entities\")\n",
    "            \n",
    "            status.update(label=\"Pipeline complete!\", state=\"complete\", expanded=False)\n",
    "        \n",
    "        return {\n",
    "            'step1_extracted': extracted,\n",
    "            'step2_grounded': grounded,\n",
    "            'step3_verified': verified,\n",
    "            'summary': f\"Pipeline: {len(extracted)} ‚Üí {len(grounded)} ‚Üí {len(verified)} entities\"\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    st.title(\"üè¢ Entity Recognition - Step 2: Advanced 3-Step Pipeline\")\n",
    "    st.markdown(\"\"\"\n",
    "    **Enhanced entity extraction with:**\n",
    "    - üîç **Step 1**: Extract potential entities with AI\n",
    "    - üóÉÔ∏è **Step 2**: Ground against company database  \n",
    "    - ‚úÖ **Step 3**: Verify with RAG-style analysis\n",
    "    \"\"\")\n",
    "    \n",
    "    # API Key\n",
    "    \n",
    "    \n",
    "    if not api_key:\n",
    "        st.warning(\"Please set your OpenAI API key to continue.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = AdvancedEntityExtractor(api_key)\n",
    "    \n",
    "    # Show company database\n",
    "    with st.expander(\"üìä View Company Database\"):\n",
    "        db_df = pd.DataFrame(extractor.company_db.get_all_companies())\n",
    "        st.dataframe(db_df, use_container_width=True)\n",
    "    \n",
    "    # Text input\n",
    "    st.subheader(\"üìù Enter Financial Text\")\n",
    "    sample_text = \"Apple (AAPL) reported strong Q4 earnings. Microsoft (MSFT) showed impressive Azure growth. Tesla (TSLA) announced milestones. Google parent Alphabet (GOOGL) saw revenue rebound. Facebook's Meta announced VR initiatives.\"\n",
    "    \n",
    "    text_input = st.text_area(\n",
    "        \"Enter or paste financial text here:\",\n",
    "        value=sample_text,\n",
    "        height=150,\n",
    "        placeholder=\"Paste financial news or article text here...\"\n",
    "    )\n",
    "    \n",
    "    if st.button(\"üöÄ Run 3-Step Pipeline\", type=\"primary\"):\n",
    "        if text_input.strip():\n",
    "            # Run the pipeline\n",
    "            result = extractor.process_text_pipeline(text_input)\n",
    "            \n",
    "            # Create tabs to show each step\n",
    "            tab1, tab2, tab3, tab4 = st.tabs([\"üìä Summary\", \"üîç Step 1: Extract\", \"üóÉÔ∏è Step 2: Ground\", \"‚úÖ Step 3: Verify\"])\n",
    "            \n",
    "            with tab1:\n",
    "                st.metric(\"Pipeline Summary\", result['summary'])\n",
    "                \n",
    "                if result['step3_verified']:\n",
    "                    st.success(f\"‚úÖ Successfully identified {len(result['step3_verified'])} companies!\")\n",
    "                    \n",
    "                    # Final results table\n",
    "                    df = pd.DataFrame(result['step3_verified'])\n",
    "                    st.subheader(\"üéØ Final Verified Companies\")\n",
    "                    st.dataframe(df, use_container_width=True)\n",
    "                    \n",
    "                    # Company cards\n",
    "                    st.subheader(\"üè∑Ô∏è Company Cards\")\n",
    "                    cols = st.columns(min(len(result['step3_verified']), 3))\n",
    "                    for i, entity in enumerate(result['step3_verified']):\n",
    "                        with cols[i % 3]:\n",
    "                            st.info(f\"\"\"**{entity['company_name']}**\n",
    "                            \n",
    "üìà **Ticker:** {entity['ticker']}\n",
    "üîç **Found as:** '{entity['original']}'\n",
    "üìä **Confidence:** {entity['confidence']}%\n",
    "üí° **Reasoning:** {entity['reasoning']}\"\"\")\n",
    "                else:\n",
    "                    st.warning(\"No companies found in the text.\")\n",
    "            \n",
    "            with tab2:\n",
    "                st.subheader(\"üîç Step 1: Raw Entity Extraction\")\n",
    "                if result['step1_extracted']:\n",
    "                    df1 = pd.DataFrame(result['step1_extracted'])\n",
    "                    st.dataframe(df1, use_container_width=True)\n",
    "                else:\n",
    "                    st.info(\"No entities extracted in step 1\")\n",
    "            \n",
    "            with tab3:\n",
    "                st.subheader(\"üóÉÔ∏è Step 2: Database Grounding\")\n",
    "                if result['step2_grounded']:\n",
    "                    # Create a simplified view for grounded entities\n",
    "                    grounded_simple = []\n",
    "                    for entity in result['step2_grounded']:\n",
    "                        grounded_simple.append({\n",
    "                            'Original': entity['original'],\n",
    "                            'Matched Company': entity['matched_company']['name'],\n",
    "                            'Ticker': entity['matched_company']['ticker'],\n",
    "                            'Match Type': entity['match_type'],\n",
    "                            'Confidence': f\"{entity['confidence']}%\"\n",
    "                        })\n",
    "                    df2 = pd.DataFrame(grounded_simple)\n",
    "                    st.dataframe(df2, use_container_width=True)\n",
    "                else:\n",
    "                    st.info(\"No entities grounded in step 2\")\n",
    "            \n",
    "            with tab4:\n",
    "                st.subheader(\"‚úÖ Step 3: RAG Verification\")\n",
    "                if result['step3_verified']:\n",
    "                    df3 = pd.DataFrame(result['step3_verified'])\n",
    "                    st.dataframe(df3, use_container_width=True)\n",
    "                else:\n",
    "                    st.info(\"No entities verified in step 3\")\n",
    "        else:\n",
    "            st.error(\"Please enter some text to analyze.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the enhanced Streamlit app\n",
    "with open('app_step2_enhanced.py', 'w') as f:\n",
    "    f.write(streamlit_app_step2_enhanced)\n",
    "\n",
    "print(\"‚úÖ Enhanced Step 2 Streamlit app created: app_step2_enhanced.py\")\n",
    "print(\"üöÄ Features the complete 3-step pipeline:\")\n",
    "print(\"   üîç Step 1: AI Entity Extraction\")\n",
    "print(\"   üóÉÔ∏è Step 2: Database Grounding\") \n",
    "print(\"   ‚úÖ Step 3: RAG-style Verification\")\n",
    "print(\"\\nüöÄ To run this enhanced app, execute in terminal:\")\n",
    "print(\"streamlit run app_step2_enhanced.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìÑ Step 3: Add PDF Processing Functionality\n",
    "\n",
    "Now let's extend our capabilities to handle PDF files. We'll add PDF text extraction using PyPDF2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a PDF processing class\n",
    "import PyPDF2\n",
    "import io\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"Process PDF files to extract text\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(pdf_file) -> str:\n",
    "        \"\"\"Extract text from uploaded PDF file\"\"\"\n",
    "        try:\n",
    "            # Read the PDF file\n",
    "            if hasattr(pdf_file, 'read'):\n",
    "                # File-like object (from Streamlit)\n",
    "                pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))\n",
    "            else:\n",
    "                # File path\n",
    "                pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            \n",
    "            # Extract text from all pages\n",
    "            text = \"\"\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            \n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Test PDF processing (we'll create a simple test)\n",
    "pdf_processor = PDFProcessor()\n",
    "print(\"‚úÖ PDF processor created!\")\n",
    "print(\"üìÑ Ready to handle PDF files in our Streamlit app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced Streamlit app with PDF support\n",
    "streamlit_app_step3 = '''\n",
    "import streamlit as st\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import io\n",
    "\n",
    "# App configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Entity Extractor - Step 3\",\n",
    "    page_icon=\"üè¢\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "class SimpleEntityExtractor:\n",
    "    \"\"\"Simple entity extractor using OpenAI API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    def extract_companies(self, text: str):\n",
    "        \"\"\"Extract company mentions from text\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Extract all company names and stock tickers from the following text.\n",
    "        Return the result as a JSON object with a \"companies\" field containing a list.\n",
    "        \n",
    "        Text: {text[:3000]}...  # Truncate for API limits\n",
    "        \n",
    "        Format: {{\"companies\": [{{\"name\": \"Company Name\", \"ticker\": \"TICK\", \"original_text\": \"original\"}}]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a financial entity extraction expert. Always respond with valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result.get('companies', [])\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error extracting entities: {e}\")\n",
    "            return []\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"Process PDF files to extract text\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(pdf_file) -> str:\n",
    "        \"\"\"Extract text from uploaded PDF file\"\"\"\n",
    "        try:\n",
    "            # Read the PDF file\n",
    "            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))\n",
    "            \n",
    "            # Extract text from all pages\n",
    "            text = \"\"\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\\\n\"\n",
    "            \n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error reading PDF: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"üè¢ Entity Recognition - Step 3: Text + PDF Processing\")\n",
    "    st.markdown(\"Extract company mentions from financial text and PDF documents using AI\")\n",
    "    \n",
    "    # API Key\n",
    "    api_key = \n",
    "    \n",
    "    if not api_key:\n",
    "        st.warning(\"Please set your OpenAI API key to continue.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize components\n",
    "    extractor = SimpleEntityExtractor(api_key)\n",
    "    pdf_processor = PDFProcessor()\n",
    "    \n",
    "    # Create tabs for different input methods\n",
    "    tab1, tab2 = st.tabs([\"üìù Text Input\", \"üìÑ PDF Upload\"])\n",
    "    \n",
    "    with tab1:\n",
    "        st.subheader(\"üìù Enter Financial Text\")\n",
    "        sample_text = \"Apple (AAPL) reported strong Q4 earnings. Microsoft (MSFT) showed impressive Azure growth. Tesla (TSLA) announced new production milestones.\"\n",
    "        \n",
    "        text_input = st.text_area(\n",
    "            \"Enter or paste financial text here:\",\n",
    "            value=sample_text,\n",
    "            height=150,\n",
    "            placeholder=\"Paste financial news or article text here...\",\n",
    "            key=\"text_input\"\n",
    "        )\n",
    "        \n",
    "        if st.button(\"üîç Extract from Text\", type=\"primary\", key=\"text_extract\"):\n",
    "            process_text(text_input, extractor)\n",
    "    \n",
    "    with tab2:\n",
    "        st.subheader(\"üìÑ Upload PDF Document\")\n",
    "        uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
    "        \n",
    "        if uploaded_file is not None:\n",
    "            if st.button(\"üîç Extract from PDF\", type=\"primary\", key=\"pdf_extract\"):\n",
    "                with st.spinner(\"Extracting text from PDF...\"):\n",
    "                    extracted_text = pdf_processor.extract_text_from_pdf(uploaded_file)\n",
    "                \n",
    "                if extracted_text:\n",
    "                    st.success(f\"‚úÖ Extracted {len(extracted_text)} characters from PDF\")\n",
    "                    \n",
    "                    # Show preview of extracted text\n",
    "                    with st.expander(\"Preview extracted text\"):\n",
    "                        st.text(extracted_text[:1000] + \"...\" if len(extracted_text) > 1000 else extracted_text)\n",
    "                    \n",
    "                    # Process the extracted text\n",
    "                    process_text(extracted_text, extractor)\n",
    "                else:\n",
    "                    st.error(\"‚ùå Could not extract text from PDF\")\n",
    "\n",
    "def process_text(text: str, extractor):\n",
    "    \"\"\"Process text and display results\"\"\"\n",
    "    if text.strip():\n",
    "        with st.spinner(\"Extracting companies...\"):\n",
    "            entities = extractor.extract_companies(text)\n",
    "        \n",
    "        if entities:\n",
    "            st.success(f\"‚úÖ Found {len(entities)} companies!\")\n",
    "            \n",
    "            # Display results in a table\n",
    "            df = pd.DataFrame(entities)\n",
    "            st.subheader(\"üìä Extracted Companies\")\n",
    "            st.dataframe(df, use_container_width=True)\n",
    "            \n",
    "            # Display as cards\n",
    "            st.subheader(\"üè∑Ô∏è Company Cards\")\n",
    "            cols = st.columns(min(len(entities), 3))\n",
    "            for i, entity in enumerate(entities):\n",
    "                with cols[i % 3]:\n",
    "                    st.info(f\"**{entity['name']}**\\\\n\\\\nTicker: {entity.get('ticker', 'N/A')}\\\\n\\\\nFound as: '{entity.get('original_text', 'N/A')}'\")\n",
    "        else:\n",
    "            st.warning(\"No companies found in the text.\")\n",
    "    else:\n",
    "        st.error(\"Please provide some text to analyze.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the enhanced Streamlit app\n",
    "with open('app_step3.py', 'w') as f:\n",
    "    f.write(streamlit_app_step3)\n",
    "\n",
    "print(\"‚úÖ Step 3 Streamlit app created: app_step3.py\")\n",
    "print(\"üìÑ Now supports both text input and PDF file uploads!\")\n",
    "print(\"\\\\nüöÄ To run this app, execute in terminal:\")\n",
    "print(\"streamlit run app_step3.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üñºÔ∏è Step 4: Add Image OCR Capabilities\n",
    "\n",
    "Now let's add the ability to extract text from images using OCR (Optical Character Recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OCR libraries\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pytesseract\n",
    "import cv2\n",
    "import base64\n",
    "\n",
    "class OCRProcessor:\n",
    "    \"\"\"Process images to extract text using OCR\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_image(image: Image.Image) -> str:\n",
    "        \"\"\"Extract text from image using OCR\"\"\"\n",
    "        try:\n",
    "            # Convert PIL Image to OpenCV format\n",
    "            img_array = np.array(image)\n",
    "            \n",
    "            # Convert to grayscale if needed\n",
    "            if len(img_array.shape) == 3:\n",
    "                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                gray = img_array\n",
    "            \n",
    "            # Apply some preprocessing for better OCR\n",
    "            alpha = 1.5  # Contrast control\n",
    "            beta = 10    # Brightness control\n",
    "            adjusted = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "            \n",
    "            # Apply threshold to get binary image\n",
    "            _, thresh = cv2.threshold(adjusted, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # Perform OCR\n",
    "            text = pytesseract.image_to_string(thresh)\n",
    "            \n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"OCR extraction failed: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "# Test OCR with a simple text image\n",
    "def create_test_image():\n",
    "    \"\"\"Create a test image with company names\"\"\"\n",
    "    img = Image.new('RGB', (400, 200), color='white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype('/System/Library/Fonts/Arial.ttf', 24)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    draw.text((20, 50), \"Apple Inc. (AAPL)\", fill='black', font=font)\n",
    "    draw.text((20, 90), \"Microsoft Corp. (MSFT)\", fill='black', font=font)\n",
    "    draw.text((20, 130), \"Tesla Inc. (TSLA)\", fill='black', font=font)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Test OCR functionality\n",
    "ocr_processor = OCRProcessor()\n",
    "test_image = create_test_image()\n",
    "\n",
    "print(\"üñºÔ∏è Testing OCR functionality...\")\n",
    "extracted_text = ocr_processor.extract_text_from_image(test_image)\n",
    "print(f\"üìù Extracted text: {repr(extracted_text)}\")\n",
    "\n",
    "if extracted_text and ('Apple' in extracted_text or 'Microsoft' in extracted_text):\n",
    "    print(\"‚úÖ OCR is working correctly!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OCR may need adjustment, but functionality is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streamlit app with OCR support\n",
    "streamlit_app_step4 = '''\n",
    "import streamlit as st\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import io\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# App configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Entity Extractor - Step 4\",\n",
    "    page_icon=\"üè¢\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "class SimpleEntityExtractor:\n",
    "    \"\"\"Simple entity extractor using OpenAI API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    def extract_companies(self, text: str):\n",
    "        \"\"\"Extract company mentions from text\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Extract all company names and stock tickers from the following text.\n",
    "        Return the result as a JSON object with a \"companies\" field containing a list.\n",
    "        \n",
    "        Text: {text[:3000]}...  # Truncate for API limits\n",
    "        \n",
    "        Format: {{\"companies\": [{{\"name\": \"Company Name\", \"ticker\": \"TICK\", \"original_text\": \"original\"}}]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a financial entity extraction expert. Always respond with valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result.get('companies', [])\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error extracting entities: {e}\")\n",
    "            return []\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"Process PDF files to extract text\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(pdf_file) -> str:\n",
    "        \"\"\"Extract text from uploaded PDF file\"\"\"\n",
    "        try:\n",
    "            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))\n",
    "            text = \"\"\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\\\n\"\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error reading PDF: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "class OCRProcessor:\n",
    "    \"\"\"Process images to extract text using OCR\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_image(image: Image.Image) -> str:\n",
    "        \"\"\"Extract text from image using OCR\"\"\"\n",
    "        try:\n",
    "            # Convert PIL Image to OpenCV format\n",
    "            img_array = np.array(image)\n",
    "            \n",
    "            # Convert to grayscale if needed\n",
    "            if len(img_array.shape) == 3:\n",
    "                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                gray = img_array\n",
    "            \n",
    "            # Apply some preprocessing for better OCR\n",
    "            alpha = 1.5  # Contrast control\n",
    "            beta = 10    # Brightness control\n",
    "            adjusted = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "            \n",
    "            # Apply threshold to get binary image\n",
    "            _, thresh = cv2.threshold(adjusted, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # Perform OCR\n",
    "            text = pytesseract.image_to_string(thresh)\n",
    "            \n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            st.error(f\"OCR extraction failed: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"üè¢ Entity Recognition - Step 4: Text + PDF + Images (OCR)\")\n",
    "    st.markdown(\"Extract company mentions from text, PDF documents, and images using AI and OCR\")\n",
    "    \n",
    "    # API Key\n",
    "    api_key = \n",
    "    \n",
    "    if not api_key:sk-\n",
    "        st.warning(\"Please set your OpenAI API key to continue.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize components\n",
    "    extractor = SimpleEntityExtractor(api_key)\n",
    "    pdf_processor = PDFProcessor()\n",
    "    ocr_processor = OCRProcessor()\n",
    "    \n",
    "    # Create tabs for different input methods\n",
    "    tab1, tab2, tab3 = st.tabs([\"üìù Text Input\", \"üìÑ PDF Upload\", \"üñºÔ∏è Image Upload\"])\n",
    "    \n",
    "    with tab1:\n",
    "        st.subheader(\"üìù Enter Financial Text\")\n",
    "        sample_text = \"Apple (AAPL) reported strong Q4 earnings. Microsoft (MSFT) showed impressive Azure growth. Tesla (TSLA) announced new production milestones.\"\n",
    "        \n",
    "        text_input = st.text_area(\n",
    "            \"Enter or paste financial text here:\",\n",
    "            value=sample_text,\n",
    "            height=150,\n",
    "            placeholder=\"Paste financial news or article text here...\",\n",
    "            key=\"text_input\"\n",
    "        )\n",
    "        \n",
    "        if st.button(\"üîç Extract from Text\", type=\"primary\", key=\"text_extract\"):\n",
    "            process_text(text_input, extractor, \"Text Input\")\n",
    "    \n",
    "    with tab2:\n",
    "        st.subheader(\"üìÑ Upload PDF Document\")\n",
    "        uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
    "        \n",
    "        if uploaded_file is not None:\n",
    "            if st.button(\"üîç Extract from PDF\", type=\"primary\", key=\"pdf_extract\"):\n",
    "                with st.spinner(\"Extracting text from PDF...\"):\n",
    "                    extracted_text = pdf_processor.extract_text_from_pdf(uploaded_file)\n",
    "                \n",
    "                if extracted_text:\n",
    "                    st.success(f\"‚úÖ Extracted {len(extracted_text)} characters from PDF\")\n",
    "                    \n",
    "                    # Show preview of extracted text\n",
    "                    with st.expander(\"Preview extracted text\"):\n",
    "                        st.text(extracted_text[:1000] + \"...\" if len(extracted_text) > 1000 else extracted_text)\n",
    "                    \n",
    "                    # Process the extracted text\n",
    "                    process_text(extracted_text, extractor, \"PDF\")\n",
    "                else:\n",
    "                    st.error(\"‚ùå Could not extract text from PDF\")\n",
    "    \n",
    "    with tab3:\n",
    "        st.subheader(\"üñºÔ∏è Upload Image\")\n",
    "        uploaded_image = st.file_uploader(\"Choose an image file\", type=[\"png\", \"jpg\", \"jpeg\", \"bmp\", \"tiff\"])\n",
    "        \n",
    "        if uploaded_image is not None:\n",
    "            # Display the uploaded image\n",
    "            image = Image.open(uploaded_image)\n",
    "            st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "            \n",
    "            if st.button(\"üîç Extract from Image (OCR)\", type=\"primary\", key=\"image_extract\"):\n",
    "                with st.spinner(\"Extracting text from image using OCR...\"):\n",
    "                    extracted_text = ocr_processor.extract_text_from_image(image)\n",
    "                \n",
    "                if extracted_text:\n",
    "                    st.success(f\"‚úÖ Extracted text from image: {len(extracted_text)} characters\")\n",
    "                    \n",
    "                    # Show preview of extracted text\n",
    "                    with st.expander(\"Preview extracted text\"):\n",
    "                        st.text(extracted_text)\n",
    "                    \n",
    "                    # Process the extracted text\n",
    "                    process_text(extracted_text, extractor, \"Image (OCR)\")\n",
    "                else:\n",
    "                    st.warning(\"‚ö†Ô∏è No text found in the image\")\n",
    "\n",
    "def process_text(text: str, extractor, source: str):\n",
    "    \"\"\"Process text and display results\"\"\"\n",
    "    if text.strip():\n",
    "        with st.spinner(\"Extracting companies...\"):\n",
    "            entities = extractor.extract_companies(text)\n",
    "        \n",
    "        if entities:\n",
    "            st.success(f\"‚úÖ Found {len(entities)} companies from {source}!\")\n",
    "            \n",
    "            # Display results in a table\n",
    "            df = pd.DataFrame(entities)\n",
    "            df['Source'] = source\n",
    "            st.subheader(\"üìä Extracted Companies\")\n",
    "            st.dataframe(df, use_container_width=True)\n",
    "            \n",
    "            # Display as cards\n",
    "            st.subheader(\"üè∑Ô∏è Company Cards\")\n",
    "            cols = st.columns(min(len(entities), 3))\n",
    "            for i, entity in enumerate(entities):\n",
    "                with cols[i % 3]:\n",
    "                    st.info(f\"**{entity['name']}**\\\\n\\\\nTicker: {entity.get('ticker', 'N/A')}\\\\n\\\\nFound as: '{entity.get('original_text', 'N/A')}'\\\\n\\\\nSource: {source}\")\n",
    "        else:\n",
    "            st.warning(f\"No companies found in the {source.lower()}.\")\n",
    "    else:\n",
    "        st.error(\"Please provide some content to analyze.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the enhanced Streamlit app\n",
    "with open('app_step4.py', 'w') as f:\n",
    "    f.write(streamlit_app_step4)\n",
    "\n",
    "print(\"‚úÖ Step 4 Streamlit app created: app_step4.py\")\n",
    "print(\"üñºÔ∏è Now supports text input, PDF files, and image OCR!\")\n",
    "print(\"\\nüöÄ To run this app, execute in terminal:\")\n",
    "print(\"streamlit run app_step4.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü§ñ Step 5: Add AI Vision Analysis\n",
    "\n",
    "Finally, let's add GPT-4 Vision capabilities to analyze charts and graphs for company information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI Vision processor\n",
    "class VisionProcessor:\n",
    "    \"\"\"Process images using AI vision to detect companies\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    def analyze_image_for_companies(self, image: Image.Image) -> Dict[str, Any]:\n",
    "        \"\"\"Use GPT-4 Vision to analyze image for company information\"\"\"\n",
    "        try:\n",
    "            # Convert image to base64\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            # Create prompt for vision analysis\n",
    "            prompt = \"\"\"Analyze this image and extract any company information present.\n",
    "            Look for:\n",
    "            1. Company names mentioned in text, labels, or titles\n",
    "            2. Stock tickers (like AAPL, MSFT, etc.)\n",
    "            3. Company logos or branding\n",
    "            4. Data labels in charts referring to companies\n",
    "            \n",
    "            Return a JSON object with:\n",
    "            {\n",
    "                \"companies_found\": [\n",
    "                    {\n",
    "                        \"name\": \"company name\",\n",
    "                        \"ticker\": \"ticker symbol if found\",\n",
    "                        \"context\": \"where/how it was found in the image\"\n",
    "                    }\n",
    "                ],\n",
    "                \"has_chart\": true/false,\n",
    "                \"chart_type\": \"bar/line/pie/other if applicable\",\n",
    "                \"description\": \"brief description of what the image shows\"\n",
    "            }\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/png;base64,{img_base64}\",\n",
    "                                    \"detail\": \"high\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Vision API analysis failed: {str(e)}\")\n",
    "            return {\n",
    "                \"companies_found\": [],\n",
    "                \"has_chart\": False,\n",
    "                \"description\": \"Failed to analyze image\"\n",
    "            }\n",
    "\n",
    "# Test vision processing with our test image\n",
    "vision_processor = VisionProcessor(API_KEY)\n",
    "print(\"ü§ñ Testing AI Vision functionality...\")\n",
    "\n",
    "# Use the same test image from OCR\n",
    "vision_result = vision_processor.analyze_image_for_companies(test_image)\n",
    "print(f\"üîç Vision analysis result: {json.dumps(vision_result, indent=2)}\")\n",
    "\n",
    "if vision_result.get('companies_found'):\n",
    "    print(f\"‚úÖ Vision AI detected {len(vision_result['companies_found'])} companies!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Vision AI didn't detect companies in test image, but functionality is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Step 6: Final Comprehensive Application\n",
    "\n",
    "Now let's create the final version that combines all capabilities: text processing, PDF handling, OCR, and AI vision analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final comprehensive Streamlit app\n",
    "streamlit_app_final = '''\n",
    "import streamlit as st\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import io\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# App configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Entity Recognition - Complete\",\n",
    "    page_icon=\"üè¢\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "class ComprehensiveEntityExtractor:\n",
    "    \"\"\"Complete entity extractor with all capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    def extract_companies(self, text: str):\n",
    "        \"\"\"Extract company mentions from text\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Extract all company names and stock tickers from the following text.\n",
    "        Return the result as a JSON object with a \"companies\" field containing a list.\n",
    "        \n",
    "        Text: {text[:3000]}...  # Truncate for API limits\n",
    "        \n",
    "        Format: {{\"companies\": [{{\"name\": \"Company Name\", \"ticker\": \"TICK\", \"original_text\": \"original\", \"confidence\": 90}}]}}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a financial entity extraction expert. Always respond with valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result.get('companies', [])\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error extracting entities: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def analyze_image_for_companies(self, image: Image.Image) -> Dict[str, Any]:\n",
    "        \"\"\"Use GPT-4 Vision to analyze image for company information\"\"\"\n",
    "        try:\n",
    "            # Convert image to base64\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            prompt = \"\"\"Analyze this image and extract any company information present.\n",
    "            Look for company names, stock tickers, logos, and data labels in charts.\n",
    "            \n",
    "            Return JSON: {\"companies_found\": [{\"name\": \"Company\", \"ticker\": \"TICK\", \"context\": \"where found\"}], \"description\": \"what the image shows\"}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/png;base64,{img_base64}\",\n",
    "                                    \"detail\": \"high\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.warning(f\"Vision analysis failed: {str(e)}\")\n",
    "            return {\"companies_found\": [], \"description\": \"Failed to analyze image\"}\n",
    "\n",
    "class PDFProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(pdf_file) -> str:\n",
    "        try:\n",
    "            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))\n",
    "            text = \"\"\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\\\n\"\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error reading PDF: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "class OCRProcessor:\n",
    "    @staticmethod\n",
    "    def extract_text_from_image(image: Image.Image) -> str:\n",
    "        try:\n",
    "            img_array = np.array(image)\n",
    "            if len(img_array.shape) == 3:\n",
    "                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                gray = img_array\n",
    "            \n",
    "            alpha = 1.5\n",
    "            beta = 10\n",
    "            adjusted = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "            _, thresh = cv2.threshold(adjusted, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            text = pytesseract.image_to_string(thresh)\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            st.warning(f\"OCR extraction failed: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "def main():\n",
    "    st.title(\"üè¢ Complete Entity Recognition System\")\n",
    "    st.markdown(\"\"\"Extract company mentions from **text**, **PDF documents**, and **images** using:\n",
    "    - ü§ñ **AI-powered text analysis**\n",
    "    - üìÑ **PDF text extraction** \n",
    "    - üñºÔ∏è **OCR (Optical Character Recognition)**\n",
    "    - üëÅÔ∏è **AI Vision analysis** for charts and graphics\n",
    "    \"\"\")\n",
    "    \n",
    "    # API Key\n",
    "    api_key = \n",
    "    \n",
    "    if not api_key:\n",
    "        st.warning(\"Please set your OpenAI API key to continue.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize all processors\n",
    "    extractor = ComprehensiveEntityExtractor(api_key)\n",
    "    pdf_processor = PDFProcessor()\n",
    "    ocr_processor = OCRProcessor()\n",
    "    \n",
    "    # Create tabs for different input methods\n",
    "    tab1, tab2, tab3 = st.tabs([\"üìù Text Input\", \"üìÑ PDF Upload\", \"üñºÔ∏è Image Analysis\"])\n",
    "    \n",
    "    with tab1:\n",
    "        st.subheader(\"üìù Enter Financial Text\")\n",
    "        sample_text = \"Apple (AAPL) reported strong Q4 earnings with revenue of $89.5B. Microsoft (MSFT) showed impressive Azure growth of 35% YoY. Tesla (TSLA) announced new production milestones, while Google parent Alphabet (GOOGL) saw advertising revenue rebound.\"\n",
    "        \n",
    "        text_input = st.text_area(\n",
    "            \"Enter or paste financial text here:\",\n",
    "            value=sample_text,\n",
    "            height=150,\n",
    "            placeholder=\"Paste financial news, earnings reports, or any text mentioning companies...\",\n",
    "            key=\"text_input\"\n",
    "        )\n",
    "        \n",
    "        if st.button(\"üîç Extract Companies from Text\", type=\"primary\", key=\"text_extract\"):\n",
    "            process_text(text_input, extractor, \"üìù Text Input\")\n",
    "    \n",
    "    with tab2:\n",
    "        st.subheader(\"üìÑ Upload PDF Document\")\n",
    "        st.markdown(\"Upload financial reports, earnings statements, or any PDF containing company information.\")\n",
    "        \n",
    "        uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
    "        \n",
    "        if uploaded_file is not None:\n",
    "            if st.button(\"üîç Extract Companies from PDF\", type=\"primary\", key=\"pdf_extract\"):\n",
    "                with st.spinner(\"Extracting text from PDF...\"):\n",
    "                    extracted_text = pdf_processor.extract_text_from_pdf(uploaded_file)\n",
    "                \n",
    "                if extracted_text:\n",
    "                    st.success(f\"‚úÖ Extracted {len(extracted_text)} characters from PDF\")\n",
    "                    \n",
    "                    with st.expander(\"Preview extracted text\"):\n",
    "                        st.text(extracted_text[:1000] + \"...\" if len(extracted_text) > 1000 else extracted_text)\n",
    "                    \n",
    "                    process_text(extracted_text, extractor, \"üìÑ PDF Document\")\n",
    "                else:\n",
    "                    st.error(\"‚ùå Could not extract text from PDF\")\n",
    "    \n",
    "    with tab3:\n",
    "        st.subheader(\"üñºÔ∏è Upload Image for Analysis\")\n",
    "        st.markdown(\"Upload charts, screenshots, infographics, or any image containing company information.\")\n",
    "        \n",
    "        uploaded_image = st.file_uploader(\n",
    "            \"Choose an image file\", \n",
    "            type=[\"png\", \"jpg\", \"jpeg\", \"bmp\", \"tiff\"],\n",
    "            help=\"Supports PNG, JPG, JPEG, BMP, and TIFF formats\"\n",
    "        )\n",
    "        \n",
    "        if uploaded_image is not None:\n",
    "            image = Image.open(uploaded_image)\n",
    "            st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                if st.button(\"üëÅÔ∏è AI Vision Analysis\", type=\"primary\", key=\"vision_extract\"):\n",
    "                    with st.spinner(\"Analyzing image with AI Vision...\"):\n",
    "                        vision_result = extractor.analyze_image_for_companies(image)\n",
    "                    \n",
    "                    st.subheader(\"ü§ñ AI Vision Results\")\n",
    "                    st.write(f\"**Description:** {vision_result.get('description', 'N/A')}\")\n",
    "                    \n",
    "                    companies = vision_result.get('companies_found', [])\n",
    "                    if companies:\n",
    "                        df = pd.DataFrame(companies)\n",
    "                        df['Source'] = 'üëÅÔ∏è AI Vision'\n",
    "                        df['Confidence'] = 85  # Default confidence for vision\n",
    "                        display_results(df, \"üëÅÔ∏è AI Vision Analysis\")\n",
    "                    else:\n",
    "                        st.info(\"No companies detected through AI vision analysis.\")\n",
    "            \n",
    "            with col2:\n",
    "                if st.button(\"üìù OCR Text Extraction\", type=\"secondary\", key=\"ocr_extract\"):\n",
    "                    with st.spinner(\"Extracting text using OCR...\"):\n",
    "                        extracted_text = ocr_processor.extract_text_from_image(image)\n",
    "                    \n",
    "                    if extracted_text:\n",
    "                        st.success(f\"‚úÖ OCR extracted {len(extracted_text)} characters\")\n",
    "                        \n",
    "                        with st.expander(\"Preview OCR text\"):\n",
    "                            st.text(extracted_text)\n",
    "                        \n",
    "                        process_text(extracted_text, extractor, \"üìù Image OCR\")\n",
    "                    else:\n",
    "                        st.warning(\"‚ö†Ô∏è No text found in the image through OCR\")\n",
    "\n",
    "def process_text(text: str, extractor, source: str):\n",
    "    \"\"\"Process text and display results\"\"\"\n",
    "    if text.strip():\n",
    "        with st.spinner(\"Extracting companies using AI...\"):\n",
    "            entities = extractor.extract_companies(text)\n",
    "        \n",
    "        if entities:\n",
    "            # Create DataFrame with source information\n",
    "            df = pd.DataFrame(entities)\n",
    "            df['Source'] = source\n",
    "            if 'confidence' not in df.columns:\n",
    "                df['Confidence'] = 90  # Default confidence\n",
    "            \n",
    "            display_results(df, source)\n",
    "        else:\n",
    "            st.warning(f\"No companies found in the {source.lower()}.\")\n",
    "    else:\n",
    "        st.error(\"Please provide some content to analyze.\")\n",
    "\n",
    "def display_results(df: pd.DataFrame, source: str):\n",
    "    \"\"\"Display extraction results\"\"\"\n",
    "    st.success(f\"‚úÖ Found {len(df)} companies from {source}!\")\n",
    "    \n",
    "    # Results table\n",
    "    st.subheader(\"üìä Extracted Companies\")\n",
    "    \n",
    "    # Reorder columns for better display\n",
    "    column_order = ['name', 'ticker', 'original_text', 'confidence', 'Source']\n",
    "    display_df = df.reindex(columns=[col for col in column_order if col in df.columns])\n",
    "    \n",
    "    st.dataframe(display_df, use_container_width=True)\n",
    "    \n",
    "    # Company cards\n",
    "    st.subheader(\"üè∑Ô∏è Company Details\")\n",
    "    cols = st.columns(min(len(df), 3))\n",
    "    \n",
    "    for i, (_, entity) in enumerate(df.iterrows()):\n",
    "        with cols[i % 3]:\n",
    "            confidence = entity.get('confidence', entity.get('Confidence', 'N/A'))\n",
    "            st.info(f\"\"\"**{entity['name']}**\n",
    "            \n",
    "üìà **Ticker:** {entity.get('ticker', 'N/A')}\n",
    "üîç **Found as:** '{entity.get('original_text', 'N/A')}'\n",
    "üìä **Confidence:** {confidence}%\n",
    "üìç **Source:** {entity.get('Source', source)}\"\"\")\n",
    "    \n",
    "    # Download option\n",
    "    csv = df.to_csv(index=False)\n",
    "    st.download_button(\n",
    "        label=\"üì• Download Results as CSV\",\n",
    "        data=csv,\n",
    "        file_name=f\"company_extraction_{source.replace(' ', '_').lower()}.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the final comprehensive Streamlit app\n",
    "with open('app_final.py', 'w') as f:\n",
    "    f.write(streamlit_app_final)\n",
    "\n",
    "print(\"‚úÖ Final comprehensive Streamlit app created: app_final.py\")\n",
    "print(\"üéØ Includes ALL features: Text, PDF, OCR, and AI Vision!\")\n",
    "print(\"\\nüöÄ To run the final app, execute in terminal:\")\n",
    "print(\"streamlit run app_final.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ Summary\n",
    "\n",
    "Congratulations! You've built a complete entity recognition system step by step. Here's what we've accomplished:\n",
    "\n",
    "## üì± Applications Created:\n",
    "\n",
    "1. **`app_step2.py`** - Basic text processing with OpenAI\n",
    "2. **`app_step3.py`** - Added PDF file support\n",
    "3. **`app_step4.py`** - Added image OCR capabilities\n",
    "4. **`app_final.py`** - Complete system with AI vision analysis\n",
    "\n",
    "## üõ†Ô∏è Technologies Used:\n",
    "\n",
    "- **OpenAI GPT-4** for text analysis and vision processing\n",
    "- **Streamlit** for web interface\n",
    "- **PyPDF2** for PDF text extraction\n",
    "- **Tesseract OCR** for image text recognition\n",
    "- **OpenCV** for image preprocessing\n",
    "- **PIL (Pillow)** for image handling\n",
    "\n",
    "## üéØ Capabilities:\n",
    "\n",
    "‚úÖ Extract companies from plain text  \n",
    "‚úÖ Process PDF documents  \n",
    "‚úÖ Read text from images (OCR)  \n",
    "‚úÖ Analyze charts and graphics with AI  \n",
    "‚úÖ Interactive web interface  \n",
    "‚úÖ Export results to CSV  \n",
    "‚úÖ Source tracking  \n",
    "‚úÖ Confidence scoring  \n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "You can now:\n",
    "- Run any of the step-by-step applications to see the progression\n",
    "- Use the final application for comprehensive entity recognition\n",
    "- Extend the system with additional features like:\n",
    "  - Entity verification against company databases\n",
    "  - Batch processing capabilities\n",
    "  - Advanced visualization\n",
    "  - API endpoints\n",
    "\n",
    "Happy coding! üéä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
